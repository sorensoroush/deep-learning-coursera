-- Why Look at Case Studies? --

@Reading or seeing other good examples of convnets is a good way to gain intuition about how to build convnets

=====================================================================================
-- Classic Networks --

LeNet-5 (1998):
	![LeCun et al., 1998, Gradient-based learning applied to document recognition]
	Size:
		32x32x1 (grayscale images, 1998)
	Goal:
		Recognize hand-written digits
	Architecture:
		32x32x1
		==(conv)==> c=6, f=5, s=1
		28x28x6
		==(avg pool)==> f=2, s=2
		14x14x6
		==(conv)==> c=16, f=5, s=1
		10x10x16
		==(avg pool)==> f=2, s=2
		5x5x16 = 400
		==(FC)==> 120 units
		==(FC)==> 84 units
		==(output)==> 10 outputs

	^Notes:
		Avg pooling was more popular
		Padding wasn't used, was always "valid" padding
		Used binaryx10 classifier (not softmax)

	Observations:	
		60k parameters (small by modern standards)
		As depth increases:
			height/width decreases
			channel# increases
		Conv+pool conv+pool FC FC output
	Advanced (if interested in reading paper):
		Used sigmoid/tanh rather than ReLU
		Filter stacking was rather complicated due to computational limitations
		Non-linearity after pooling
		Most of the information is in sections 2+3
		Later sections talk about other ideas not applicable to today

AlexNet (2012):
	![Krizhevsky et al, 2012. ImageNet classification with deep convolutional neural networks]
	Goal:
	Architecture:
		227x227x3 input
		==(conv)==> c=96, f=11, s=4
		55x55x96
		==(max pool)==> f=3, s=2
		27x27x96
		==(conv)==> c=256, f=5, s=1, padding='same'
		27x27x256
		==(max pool)==> f=3, s=2
		13x13x256
		==(conv)==> c=384, f=3, s=1, padding='same'
		13x13x384
		==(conv)==> c=384, f=3, s=1, padding='same'
		13x13x384
		==(conv)==> c=256, f=3, s=1, padding='same'
		13x13x256
		==(max pool)==> f=3, s=2
		6x6x256 = 9216
		==(FC)==> 4096 units
		==(FC)==> 4096 units
		==(softmax)==> 1000 outputs
	
	Observations:
		Similiar to LeNet but much bigger
			(60k parameters vs 60M parameters)
		ReLU was used

	Advanced:
		Layers were split among multiple GPUs
		Another kind of layer called "Local Response Normalization"
			Look across the channels and normalize them
			Researchers later realized this doesn't improve performance
	
	!This was the paper that convinced the computer vision community of the potential of deep learning


VGG-16 (2015):
	[Simonyan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition]
	"Instead of hyperparameter overload let's make it simple"
	==(conv)==> f=3, s=1, padding='same'
	==(max pool)==> f=2, s=2
	Architecture:
		224x224x3
		==(conv x2)==> c=64
		224x224x64
		==(pool)==>
		112x112x64
		==(conv x2)==> c=128
		112x112x128
		==(pool)==>
		56x56x128
		==(conv x3)==> c=256
		56x56x256
		==(pool)==>
		28x28x256	
		==(conv x3)==> c=512
		28x28x512	
		==(pool)==>
		14x14x512
		==(conv x3)==> c=512
		14x14x512
		==(pool)==>
		7x7x512
		==(FC)==> 4096 units
		==(FC)==> 4096 units
		==(softmax)==> 1000 outputs

	Observations:
		~138M parameters
			large even by modern standards
		Appealing due to architectural simplicity and uniformity (no hyperparameter overload)
		Doubling of filter#
	
	Advanced:
		VGG-19 is mentioned, a bigger version of this

=====================================================================================
-- ResNets --

Built from Residual Blocks:
	Essentially an earlier activation being added to a later pre-activation
		a[l+2] = g(z[l+2] + a[l])
		Called "shortcuts" or skip connections

To turn a "plain" network into resnet:
	Stack residual blocks
		@Fastforward the input of a layer into the l+2 activation; repeat in sequence	

In a plain network:
	Training error eventually increases as layer# increases
		This occurs because the optimization algorithm has difficulty training a network that is too deep

In a ResNet:
	Training error will not increase with depth
		The residual blocks relieves the algorithm of vanishing gradients

=====================================================================================
-- Why ResNets Work? --

a[l+2] = g(w[l+2] * a[l+1] + b[l+2] + a[l])
	If w and b vanish to 0:
		a[l+2] = g(a[l])

^Similar in concept to adding epsilon to denominator of optimization algorithm:
	Adds a "backup" value so gradients aren't near-zero

The identity function is easy for the network to learn, as a backup
	Gets the added benefit of a deeper network without the liability of vanishing gradients
		"It never hurts, it can only help"
		Whereas deep plain nets will struggle to learn anything, even the identity function

!ResNets are contingent on matching dimensions inside residual blocks
	As a result "same" convolutions are commonly used inside a network to allow for residual blocks
	!If the dimensions are not matching:
		Implement Ws to be multiplied to a[l] before adding it to z[l+2]
			Ws could be a matrix of parameters to be learned
			Ws could be a fixed matrix to bad a[l] with zeros
	
=====================================================================================
-- Networks in Networks and 1x1 Convolutions --

Does exactly what you expect with an FxFx1 input:
	Simple multiplication; not very useful

With an FxFxc input:
	A 1x1xc filter acts as a "network in network"; could be called a sort of FC layer
		Takes the # of channels and maps them to a single value
	FxFxc ==(1x1xc x #f)==> FxFx#f

1x1 convolutions are good for:
	Maintaining height and width 
		while
	Shrinking # of channels
	(If # of channels are maintained, functions to add a layer of non-linearity (complexity))

	Allows to save on computation in some networks

=====================================================================================
-- Inception Network Motivation --

Inception Network Layer:
	"Let's do them all at the same time!"
		Let the network learn what it wants
	Stacked together:
		1x1 convolution
		3x3 (same) convolution
		5x5 (same) convolution
		Max-pool (same; s=1)
			Very unusual pooling in order to make dimensions match
	28x28x192 ==> 28x28x256

^This is very computationally expensive:
	The 5x5 convolution alone:
		28x28x32 output volume
		x
		5x5x192 sized filters
		=
		120M multiplications

To reduce 5x5 conv computation:
	28x28x192
	==(1x1 conv, #f=16, 1x1x192)==>
	28x28x16 ("bottleneck layer")
	==(5x5 conv, #f=32, 5x5x16)==>
	28x28x32

	First conv is:
		28x28x16
		x	
		192
		=
		2.4M multiplations
	
	Second conv is:
		28x28x32
		x
		5x5x16
		=
		10M multiplcations

	= 12.4 multiplications
		(this is 1/10th of the original)

@As long as the this bottleneck is implemented, inception networks do not hurt performance

=====================================================================================
-- Inception Network --

|

=====================================================================================
-- MobileNet --

|

=====================================================================================
-- MobileNet Architecture --

|

=====================================================================================
-- EfficientNet --

|

=====================================================================================
