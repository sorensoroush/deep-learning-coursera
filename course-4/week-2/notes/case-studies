-- Why Look at Case Studies? --

@Reading or seeing other good examples of convnets is a good way to gain intuition about how to build convnets

=====================================================================================
-- Classic Networks --

LeNet-5 (1998):
	![LeCun et al., 1998, Gradient-based learning applied to document recognition]
	Size:
		32x32x1 (grayscale images, 1998)
	Goal:
		Recognize hand-written digits
	Architecture:
		32x32x1
		==(conv)==> c=6, f=5, s=1
		28x28x6
		==(avg pool)==> f=2, s=2
		14x14x6
		==(conv)==> c=16, f=5, s=1
		10x10x16
		==(avg pool)==> f=2, s=2
		5x5x16 = 400
		==(FC)==> 120 units
		==(FC)==> 84 units
		==(output)==> 10 outputs

	^Notes:
		Avg pooling was more popular
		Padding wasn't used, was always "valid" padding
		Used binaryx10 classifier (not softmax)

	Observations:	
		60k parameters (small by modern standards)
		As depth increases:
			height/width decreases
			channel# increases
		Conv+pool conv+pool FC FC output
	Advanced (if interested in reading paper):
		Used sigmoid/tanh rather than ReLU
		Filter stacking was rather complicated due to computational limitations
		Non-linearity after pooling
		Most of the information is in sections 2+3
		Later sections talk about other ideas not applicable to today

AlexNet (2012):
	![Krizhevsky et al, 2012. ImageNet classification with deep convolutional neural networks]
	Goal:
	Architecture:
		227x227x3 input
		==(conv)==> c=96, f=11, s=4
		55x55x96
		==(max pool)==> f=3, s=2
		27x27x96
		==(conv)==> c=256, f=5, s=1, padding='same'
		27x27x256
		==(max pool)==> f=3, s=2
		13x13x256
		==(conv)==> c=384, f=3, s=1, padding='same'
		13x13x384
		==(conv)==> c=384, f=3, s=1, padding='same'
		13x13x384
		==(conv)==> c=256, f=3, s=1, padding='same'
		13x13x256
		==(max pool)==> f=3, s=2
		6x6x256 = 9216
		==(FC)==> 4096 units
		==(FC)==> 4096 units
		==(softmax)==> 1000 outputs
	
	Observations:
		Similiar to LeNet but much bigger
			(60k parameters vs 60M parameters)
		ReLU was used

	Advanced:
		Layers were split among multiple GPUs
		Another kind of layer called "Local Response Normalization"
			Look across the channels and normalize them
			Researchers later realized this doesn't improve performance
	
	!This was the paper that convinced the computer vision community of the potential of deep learning


VGG-16 (2015):
	[Simonyan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition]
	"Instead of hyperparameter overload let's make it simple"
	==(conv)==> f=3, s=1, padding='same'
	==(max pool)==> f=2, s=2
	Architecture:
		224x224x3
		==(conv x2)==> c=64
		224x224x64
		==(pool)==>
		112x112x64
		==(conv x2)==> c=128
		112x112x128
		==(pool)==>
		56x56x128
		==(conv x3)==> c=256
		56x56x256
		==(pool)==>
		28x28x256	
		==(conv x3)==> c=512
		28x28x512	
		==(pool)==>
		14x14x512
		==(conv x3)==> c=512
		14x14x512
		==(pool)==>
		7x7x512
		==(FC)==> 4096 units
		==(FC)==> 4096 units
		==(softmax)==> 1000 outputs

	Observations:
		~138M parameters
			large even by modern standards
		Appealing due to architectural simplicity and uniformity (no hyperparameter overload)
		Doubling of filter#
	
	Advanced:
		VGG-19 is mentioned, a bigger version of this

=====================================================================================
-- ResNets --

[He et al., 2015. Deep residual networks for image recognition]
Built from Residual Blocks:
	Essentially an earlier activation being added to a later pre-activation
		a[l+2] = g(z[l+2] + a[l])
		Called "shortcuts" or skip connections

To turn a "plain" network into resnet:
	Stack residual blocks
		@Fastforward the input of a layer into the l+2 activation; repeat in sequence	

In a plain network:
	Training error eventually increases as layer# increases
		This occurs because the optimization algorithm has difficulty training a network that is too deep

In a ResNet:
	Training error will not increase with depth
		The residual blocks relieves the algorithm of vanishing gradients

=====================================================================================
-- Why ResNets Work? --

a[l+2] = g(w[l+2] * a[l+1] + b[l+2] + a[l])
	If w and b vanish to 0:
		a[l+2] = g(a[l])

^Similar in concept to adding epsilon to denominator of optimization algorithm:
	Adds a "backup" value so gradients aren't near-zero

The identity function is easy for the network to learn, as a backup
	Gets the added benefit of a deeper network without the liability of vanishing gradients
		"It never hurts, it can only help"
		Whereas deep plain nets will struggle to learn anything, even the identity function

!ResNets are contingent on matching dimensions inside residual blocks
	As a result "same" convolutions are commonly used inside a network to allow for residual blocks
	!If the dimensions are not matching:
		Implement Ws to be multiplied to a[l] before adding it to z[l+2]
			Ws could be a matrix of parameters to be learned
			Ws could be a fixed matrix to pad a[l] with zeros
	
=====================================================================================
-- Networks in Networks and 1x1 Convolutions --

[Lin et al., 2013. Network in network]
Does exactly what you expect with an FxFx1 input:
	Simple multiplication; not very useful

With an FxFxc input:
	A 1x1xc filter acts as a "network in network"; could be called a sort of FC layer
		Takes the # of channels and maps them to a single value
	FxFxc ==(1x1xc x #f)==> FxFx#f

1x1 convolutions are good for:
	Maintaining height and width 
		while
	Shrinking # of channels
	(If # of channels are maintained, functions to add a layer of non-linearity (complexity))

	Allows to save on computation in some networks

=====================================================================================
-- Inception Network Motivation --

[Szegedy et al. 2014. Going deeper with convolutions]
Inception Network Layer:
	"Let's do them all at the same time!"
		Let the network learn what it wants
	Stacked together:
		1x1 convolution
		3x3 (same) convolution
		5x5 (same) convolution
		Max-pool (same; s=1)
			Very unusual pooling in order to make dimensions match
	28x28x192 ==> 28x28x256

^This is very computationally expensive:
	The 5x5 convolution alone:
		28x28x32 output volume
		x
		5x5x192 sized filters
		=
		120M multiplications

To reduce 5x5 conv computation:
	28x28x192
	==(1x1 conv, #f=16, 1x1x192)==>
	28x28x16 ("bottleneck layer")
	==(5x5 conv, #f=32, 5x5x16)==>
	28x28x32

	First conv is:
		28x28x16
		x	
		192
		=
		2.4M multiplations
	
	Second conv is:
		28x28x32
		x
		5x5x16
		=
		10M multiplcations

	= 12.4 multiplications
		(this is 1/10th of the original)

@As long as the this bottleneck is implemented, inception networks do not hurt performance

=====================================================================================
-- Inception Network --

Inception module:
	Previous activation (28x28x192)
	Four step in parallel:
		1x1 conv
			output: 28x28x64
		3x3 conv
			preceded by: 
				1x1 conv (96 filters)
			output: 28x28x128
		5x5 conv
			preceded by: 
				1x1 conv (16 filters)
			output: 28x28x32
		Maxpool (3x3, s=1, 'same')
			proceeded by:
				1x1 conv (32 filters)
			output: 28x28x32
	Four steps concatenation (28x28x256)

!An inception network is essentially a chain of inception modules
	*There are also "side branches" in the noted research paper:
		Essentially FC layers stemming off of different depths followed by a softmax output
		These ensure that the intermediate features themselves are good for predictions

*The name "inception network" literally comes from the meme

=====================================================================================
-- MobileNet --

[Howard et al. 2017, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications]
Motivation:
	Computationally inexpensive
	Useful for mobile applications

Key idea:
	Depthwise separable convolutions are less computationally expensive than normal computations

Normal convolution computational cost:
	#filter params x #filter positions x #filters
	3x3x3 x 4x4 x 5
	= 2160
	(dimensions of filters x dimensions of output volume)
	

Depthwise separable convolution (2 steps):
	Depthwise convolution:
		Individually convolve every	n_C dimension along the input volume 
	Pointwise convolution:
		Take n_C' (5) filters (1x1x3) and convolve them along intermediate volume

Depthwise separable convolution computation cost:
	Depthwise convolution:
		#filter params x #filter positions x #filters
		3x3 x 4x4 x 3
		= 432
		?(Essentially splitting the filter dimensions into their own filters?)
	Pointwise convolution:
		#filter params x #filter positions x #filters
		1x1x3 x 4x4 x 5
		= 240
	= 672
		
The ratio of depthwise separable convolution to normal convolution:
	= (1/n_C') + (1/f**2)

=====================================================================================
-- MobileNet Architecture --

[Sandler et al. 2019, MobileNetV2: Inverted Residuals and Linear Bottlenecks]

MobileNet v1 repeats dsconv layers 13 times (followed by pool, fc, softmax)

MobileNet V2 repeats dsconv layers 17 times, with two main changes to the architecture:
	Residual connection from input to output of ds convo layer
	Adds an "expansion" operation to the depthwise and projection operations
		Called a "bottleneck" block
	
!MobileNet V2 bottleneck
	nxnx3 input
		(residual connection)
	expansion operation:
		18 1x1x3 filters
		outputs: nxnx8
		(typically a factor of 6 is used)
	depthwise convolution:
		outputs: nxnx18
	pointwise convolution:
		3 1x1x18 filters
		outputs: nxnx3
	nxnx3 output

Bottleneck blocks accomplish two things:
	expansion allows for more parameters to be learned, which is ideal in neural networks
	projection allows for a more compact output, which is ideal for limited-memory devices

=====================================================================================
-- EfficientNet --

[Tan and Le, 2019, EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks]
EfficientNet gives us a way to automatically scale up or down networks according to unique computation capacities

There are three variables to scale networks up/down are:
	Image resolution (r)
	Network depth (d)
	Layer width (w)

What are the rates these variables are to be scaled with computational budgets?

!^If wanting to implement EfficientNet, look up open source implementations

=====================================================================================
