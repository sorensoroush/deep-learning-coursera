-- Computer Vision --

Previously, we would be doing computer vision problems on very low-resolution images
	This is done by having every pixel be 3 input features (for 3 color channels)

If we were to take the same approach with high-resolution images:
	1000x1000 pixel images 	
	==> 3 million input features
	==> 3 billion input parameters (if you have the first hidden layer have 1000 nodes: W1 is (3000000,1000)

^As we can see, it would be infeasible to calculate every pixel
	Therefore, we need to introduce a new idea into our neural networks: convolution operations

=====================================================================================
-- Edge Detection Example --

!Edge detection is essentially:
	Image *(convolution)* filter/kernel = convolutional image
	@Tests for:
		Light on one side, dark on other (direction/orientation is determined by filter)
	
!A convolution is:
	2d-summation of element-wise multiplication of every instance of a filter size in an image
		7x7 conv2d 3x3 = 5x5
		6x6 conv2d 3x3 = 4x4 
	If there is an edge, the total convolutional image will highlight it

=====================================================================================
-- More Edge Detection --

In the convolutional image, there is a distinction of "positive" and "negative" edges.

In DL literature there are several different filter designs

!It can be better to let the algorithm figure out the best filter design, rather than using a pre-conceived design
	!This can be done by making each "pixel" of a filter into a trainable parameter

=====================================================================================
-- Padding --

There are two downsides to the previous conception of applying filters to images:
	Images shrink on every iteration; eventually will collapse
	Edge pixels are used relatively infrequently; this is discarding information

!^You resolve both of these by padding:
	@Adding extra pixels around the image to be convolved on

How much to pad? Two common choices:
	Valid convolutions:
		No padding
	Same convolutions:
		Pad so that output size is the same as the input size

*Filter size (f) is almost always odd, because:
	If f is even, need asymmetric padding
	Odd grids/matrices have centers; this can be useful for talking about filter position

=====================================================================================
-- Strided Convolutions --

@Convoluting by a specific number of steps, rather than the previous 1 (every pixel position) 

!A strided convolution must have the filter lie entirely within the image region (cannot "hang" outside)

!With:
	nxn image
	fxf filter
	p padding
	s stride

	The output size will be:
		floor(((n+2p-f)/s)+1) x floor(((n+2p-f)/s)+1)

*By convention in DL, there is no filter "flipping" in convolution (technically cross-correlation); despite there being flipping operations in math textbooks
	In math, mirroring along the origi allows for associativity in sequences of convolutions
	^But in DL, this is not important so it is omitted

=====================================================================================
-- Convolutions Over Volume --

!To convolve a 3d image, use a 3d filter
	Output is 2d

Computationally this is the same, except:
	f^3 (27 if f=3) numbers are added together, rather than f^2 (9 if f=2)

!This allows for the filter to check for edges for specific colors (or all: grayscale)
	By convention, image # of channels = filter # of channels

!When wanting to output a feature with multiple edges, desired edges are stacked together (e.g. vertical + horizontal)

=====================================================================================
-- One Layer of a Convolutional Network --

Process:
	Image
	===> conv(filter1) ===> ReLU(output + b1) ===> output
	===> conv(filter2) ===> ReLU(output + b2) ===> output
	======> 2channel_output

!One enormous advantage of CNNs is that layer parameters are not determined by image size, only filter number and size

If layer l is a convolution layer:
	f[l] = filter size
	p[l] = padding
	s[l] = stride
	nc[l] = number of filters

	Each filter is: f[l] * f[l] * nc[l-1]
	Activations: a[l] --> nh[l] * nw[l] * nc[l] 
		A[l] --> m * nh[l] * nw[l] * nc[l] 
	Weights: f[l] * f[l] * nc[l-1] * nc[l]
	Bias: nc[l]

	input: nh[l-1] * nw[l-1] * nc[l-1]
	output: nh[l] * nw[l] * nc[l]

	n[l] = floor(((n[l-1] + 2*p[l] - f[l]) / s[l]) + 1)
	^Calculate this separately for height (nh) and width (nw) 

=====================================================================================
-- Simple Convolutional Network Example --

Example ConvNet:
	x = 39 * 39 * 3
	nh[0] = nw[0] = 39
	nc[0] = 3
		f[1] = 3
		s[1] = 1
		p[1] = 0 (valid)
		10 filters

	a[1] = 37 * 37 * 10
	nh[1] = nw[1] = 37
	nc[1] = 10
		f[2] = 5
		s[2] = 2
		p[2] = 0 (valid)
		20 filters

	a[2] = 17 * 17 * 20
	nh[2] = nw[2] = 17
	nc[2] = 20
		f[3] = 5
		s[3] = 2
		p[3] = 0 (valid)
		40 filters
	
	a[3] = 7 * 7 * 40
	===> unroll this into 1960d vector
	===> logistic/softmax activation
	===> yh

^As we can see, generally:
	Height/width steadily decreases
	Channel # steadily increases

!Types of layers in CNN:
	Convolution (conv)
	Pooling (pool)
	Fully connected (fc)

=====================================================================================
-- Pooling Layers --

Max pooling:
	4x4 ==> 4 2x2s
	Take the max of each 2x2
	Output a 2x2 with each max

	@Filtering with hyperparameters:
		f = 2
		s = 2

	!What this does:
		Detect features of high interest in each region

	!Pooling layers have no parameters to learn
	!Pooling uses the same formula as convolution outputs

Average pooling:
	!Not commonly used, with one exception:
		Deep in a network, average pooling to collapse representations
	@Does what you expect:
		Averages every region instead of taking max

!Hyperparameters of pooling:
	f: filter size
	s: stride
	Max or average pooling
		
=====================================================================================
-- CNN Example --

!Two conventions with CNN layer scheme:
	conv and pool are separate layers
	conv and pool are one layer

	@Because pooling has no parameters/weights, we will use the convention of calling conv+pool one layer

Example: (inspired by LeNet-5)
	32x32x3 image input
		convolution:
			f = 5
			s = 1
			6 filters
	28x28x6 conv1
		pooling:
			max
			f = 2
			s = 2
	14x14x6 pool1
		convolution:
			f = 5
			s = 1
			10 filters	
	10x10x16 conv2
		pooling:
			max
			f = 2
			s = 2
	5x5x16 pool2
		flatting:
			400d vector
	120-unit FC3
		W[3] = (120,400)
		b[3] = (120,1)
	84-unit FC4
		W[4] = (84,120)
		b[4] = (84,1)
	Softmax (10 outputs)

=====================================================================================
-- Why Convolutions? --

!Two main advantages of convolutional layers:
	Parameter sharing
		!@A feature that's useful in one part of the image is probably useful in other parts of the image
	Sparsity of connections
		!@In each layer, each output value only depends on a small number of inputs (outputs are localized)
			~Opposite corners of an image have nothing to do with each other; but with traditional NNs they are treated as so
	
	^Through these two mechanisms, a neural network has:
		Fewer parameters to train
		Less reliance on large training sets
		Less prone to overfitting
	
!CNNs are also translation invariance
	Meaning rotations/translations do not affect outputs

!Best to try tested conv/pool hyperparameters found in literature than to experiment

=====================================================================================
