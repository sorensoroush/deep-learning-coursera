-- Mini-batch Gradient Descent --



===============================================================================
-- Understanding Mini-batch Gradient Descent --



===============================================================================
-- Exponentially Weighted Averages --



===============================================================================
-- Understanding Exponentially Weighted Averages --



===============================================================================
-- Bias Correction in Exponentially Weighted Averages --



===============================================================================
-- Gradient Descent with Momentum --



===============================================================================
-- RMSprop --



===============================================================================
-- Adam Optimization Algorithm --



===============================================================================
-- Learning Rate Decay --



===============================================================================
-- The Problem of Local Optima --



===============================================================================
